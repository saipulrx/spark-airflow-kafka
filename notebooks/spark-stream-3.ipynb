{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from kafka import KafkaProducer\n",
    "from faker import Faker\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dibimbing-jupyter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dibimbing Spark-Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff855b8b20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Ruang Data Project Spark-Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .schema(dataSchema)\n",
    "    .option('maxFilesPerTrigger', 1)\n",
    "    .json('/resources/data/activity-data/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activityCounts = streaming.select('index').distinct()\n",
    "activityQuery = (\n",
    "    activityCounts.writeStream\n",
    "    .queryName('activity_counts_3')\n",
    "    .format('memory')\n",
    "    .outputMode('append')\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# activityQuery.awaitTermination()\n",
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  290684|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  320527|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  340258|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  353626|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  362586|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT COUNT(*) FROM activity_counts_3\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - Kafka Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_path = Path('/resources/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_host = os.getenv('KAFKA_HOST')\n",
    "kafka_topic = os.getenv('KAFKA_TOPIC_NAME')\n",
    "kafka_topic_partition = os.getenv('KAFKA_TOPIC_NAME')+\"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f'{kafka_host}:9092')\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "| key|               value|     topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16381|2025-01-19 00:42:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16382|2025-01-19 00:42:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16383|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16384|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16385|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16386|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16387|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16388|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16389|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16390|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16391|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16392|2025-01-19 00:45:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16393|2025-01-19 00:45:...|            0|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "| key|               value|     topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "|null|{\"emp_id\": \"b6b86...|test-topic|        0| 16381|2025-01-19 00:42:...|            0|\n",
      "|null|{\"emp_id\": \"9aa3c...|test-topic|        0| 16382|2025-01-19 00:42:...|            0|\n",
      "|null|{\"emp_id\": \"2df54...|test-topic|        0| 16383|2025-01-19 00:43:...|            0|\n",
      "|null|{\"emp_id\": \"53661...|test-topic|        0| 16384|2025-01-19 00:43:...|            0|\n",
      "|null|{\"emp_id\": \"dcdf9...|test-topic|        0| 16385|2025-01-19 00:43:...|            0|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"emp_id\": \"b6b86170-bdc0-4ffc-8eac-ad050509bf0b\", \"employee_name\": \"Charles Burgess\", \"department\": \"Marketing\", \"state\": \"IL\", \"salary\": 16180, \"age\": 21, \"bonus\": 71805, \"ts\": 1639047747}'),\n",
       " Row(value='{\"emp_id\": \"9aa3c7c3-4c65-457f-808a-02da577361e3\", \"employee_name\": \"Paula Haynes\", \"department\": \"Marketing\", \"state\": \"CA\", \"salary\": 36555, \"age\": 24, \"bonus\": 60555, \"ts\": 1071562864}'),\n",
       " Row(value='{\"emp_id\": \"2df54166-8549-4aa5-a0cf-ae9b58d0e6fa\", \"employee_name\": \"Leslie Carson\", \"department\": \"Marketing\", \"state\": \"IL\", \"salary\": 128007, \"age\": 60, \"bonus\": 18210, \"ts\": 1102291925}'),\n",
       " Row(value='{\"emp_id\": \"53661acb-0696-4534-a3f4-259b3d034c95\", \"employee_name\": \"Mrs. Cathy Fernandez\", \"department\": \"Marketing\", \"state\": \"NY\", \"salary\": 91764, \"age\": 46, \"bonus\": 10094, \"ts\": 1198743107}'),\n",
       " Row(value='{\"emp_id\": \"dcdf90ae-6acc-44c8-b958-3a83d19ac60a\", \"employee_name\": \"Sara Mcdonald\", \"department\": \"Sales\", \"state\": \"TX\", \"salary\": 117205, \"age\": 37, \"bonus\": 96215, \"ts\": 811795364}')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    kafka_json_df\n",
    "    .select('value')\n",
    "    .limit(5)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"emp_id\", StringType(), True),\n",
    "        StructField(\"employee_name\", StringType(), True),\n",
    "        StructField(\"department\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"salary\", LongType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"bonus\", LongType(), True),\n",
    "        StructField(\"ts\", LongType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "|              emp_id|       employee_name|department|state|salary|age|bonus|        ts|\n",
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "|b6b86170-bdc0-4ff...|     Charles Burgess| Marketing|   IL| 16180| 21|71805|1639047747|\n",
      "|9aa3c7c3-4c65-457...|        Paula Haynes| Marketing|   CA| 36555| 24|60555|1071562864|\n",
      "|2df54166-8549-4aa...|       Leslie Carson| Marketing|   IL|128007| 60|18210|1102291925|\n",
      "|53661acb-0696-453...|Mrs. Cathy Fernandez| Marketing|   NY| 91764| 46|10094|1198743107|\n",
      "|dcdf90ae-6acc-44c...|       Sara Mcdonald|     Sales|   TX|117205| 37|96215| 811795364|\n",
      "|72cbfc4e-d9c8-44b...|         Brian Henry|        IT|   RJ| 26584| 20|20444|1102492295|\n",
      "|95f8944b-48a5-406...|      Evelyn Hampton|        IT|   RJ| 27185| 40|14750|1507670466|\n",
      "|a60e913d-f217-4e2...|      Joseph Griffin|        IT|   CA| 63632| 54|33758|1026610775|\n",
      "|d929b74e-325b-40f...|          Eric Jones|        IT|   NY|133685| 59|44636|1625113254|\n",
      "|4618ed6c-5c18-4f8...|      Connie Carlson|        HR|   FL| 80175| 54|59267|1252034639|\n",
      "|7b16c797-d007-474...|   Matthew Mccormick|     Sales|   RJ| 87586| 20|65558|1438642154|\n",
      "|22e61ad1-9b30-4c4...|     Jennifer Walker| Marketing|   CA| 56077| 25|21944|1586737722|\n",
      "|b8e9e7ff-1034-4dd...|      Rebecca Harvey|        HR|   IL| 10484| 57|72672|1079490827|\n",
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "(\n",
    "    kafka_json_df\n",
    "    .select(\n",
    "        from_json(col(\"value\"), schema)\n",
    "        .alias(\"data\")\n",
    "    )\n",
    "    .select(\"data.*\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stream Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f'{kafka_host}:9092')\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "parsed_df = (\n",
    "    kafka_df\n",
    "    .withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    .select(\n",
    "        from_json(col(\"value\"), schema)\n",
    "        .alias(\"data\")\n",
    "    )\n",
    "    .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    parsed_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    # .trigger(processingTime='5 seconds')\n",
    "    # .trigger(continuous='1 second')\n",
    "    # .trigger(once=true)\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
