{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from kafka import KafkaProducer\n",
    "from faker import Faker\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRuang Data Project Spark-Kafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.streaming.stopGracefullyOnShutdown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Ruang Data Project Spark-Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .schema(dataSchema)\n",
    "    .option('maxFilesPerTrigger', 1)\n",
    "    .json('/resources/data/activity-data/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activityCounts = streaming.select('index').distinct()\n",
    "activityQuery = (\n",
    "    activityCounts.writeStream\n",
    "    .queryName('activity_counts_3')\n",
    "    .format('memory')\n",
    "    .outputMode('append')\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# activityQuery.awaitTermination()\n",
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  290684|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  320527|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  340258|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  353626|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  362586|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT COUNT(*) FROM activity_counts_3\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - Kafka Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_path = Path('/resources/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_host = os.getenv('KAFKA_HOST')\n",
    "kafka_topic = os.getenv('KAFKA_TOPIC_NAME')\n",
    "kafka_topic_partition = os.getenv('KAFKA_TOPIC_NAME')+\"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f'{kafka_host}:9092')\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "| key|               value|     topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16381|2025-01-19 00:42:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16382|2025-01-19 00:42:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16383|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16384|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16385|2025-01-19 00:43:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16386|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16387|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16388|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16389|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16390|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16391|2025-01-19 00:44:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16392|2025-01-19 00:45:...|            0|\n",
      "|null|[7B 22 65 6D 70 5...|test-topic|        0| 16393|2025-01-19 00:45:...|            0|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "| key|               value|     topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "|null|{\"emp_id\": \"b6b86...|test-topic|        0| 16381|2025-01-19 00:42:...|            0|\n",
      "|null|{\"emp_id\": \"9aa3c...|test-topic|        0| 16382|2025-01-19 00:42:...|            0|\n",
      "|null|{\"emp_id\": \"2df54...|test-topic|        0| 16383|2025-01-19 00:43:...|            0|\n",
      "|null|{\"emp_id\": \"53661...|test-topic|        0| 16384|2025-01-19 00:43:...|            0|\n",
      "|null|{\"emp_id\": \"dcdf9...|test-topic|        0| 16385|2025-01-19 00:43:...|            0|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"emp_id\": \"b6b86170-bdc0-4ffc-8eac-ad050509bf0b\", \"employee_name\": \"Charles Burgess\", \"department\": \"Marketing\", \"state\": \"IL\", \"salary\": 16180, \"age\": 21, \"bonus\": 71805, \"ts\": 1639047747}'),\n",
       " Row(value='{\"emp_id\": \"9aa3c7c3-4c65-457f-808a-02da577361e3\", \"employee_name\": \"Paula Haynes\", \"department\": \"Marketing\", \"state\": \"CA\", \"salary\": 36555, \"age\": 24, \"bonus\": 60555, \"ts\": 1071562864}'),\n",
       " Row(value='{\"emp_id\": \"2df54166-8549-4aa5-a0cf-ae9b58d0e6fa\", \"employee_name\": \"Leslie Carson\", \"department\": \"Marketing\", \"state\": \"IL\", \"salary\": 128007, \"age\": 60, \"bonus\": 18210, \"ts\": 1102291925}'),\n",
       " Row(value='{\"emp_id\": \"53661acb-0696-4534-a3f4-259b3d034c95\", \"employee_name\": \"Mrs. Cathy Fernandez\", \"department\": \"Marketing\", \"state\": \"NY\", \"salary\": 91764, \"age\": 46, \"bonus\": 10094, \"ts\": 1198743107}'),\n",
       " Row(value='{\"emp_id\": \"dcdf90ae-6acc-44c8-b958-3a83d19ac60a\", \"employee_name\": \"Sara Mcdonald\", \"department\": \"Sales\", \"state\": \"TX\", \"salary\": 117205, \"age\": 37, \"bonus\": 96215, \"ts\": 811795364}')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    kafka_json_df\n",
    "    .select('value')\n",
    "    .limit(5)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"emp_id\", StringType(), True),\n",
    "        StructField(\"employee_name\", StringType(), True),\n",
    "        StructField(\"department\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"salary\", LongType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"bonus\", LongType(), True),\n",
    "        StructField(\"ts\", LongType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "|              emp_id|       employee_name|department|state|salary|age|bonus|        ts|\n",
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "|b6b86170-bdc0-4ff...|     Charles Burgess| Marketing|   IL| 16180| 21|71805|1639047747|\n",
      "|9aa3c7c3-4c65-457...|        Paula Haynes| Marketing|   CA| 36555| 24|60555|1071562864|\n",
      "|2df54166-8549-4aa...|       Leslie Carson| Marketing|   IL|128007| 60|18210|1102291925|\n",
      "|53661acb-0696-453...|Mrs. Cathy Fernandez| Marketing|   NY| 91764| 46|10094|1198743107|\n",
      "|dcdf90ae-6acc-44c...|       Sara Mcdonald|     Sales|   TX|117205| 37|96215| 811795364|\n",
      "|72cbfc4e-d9c8-44b...|         Brian Henry|        IT|   RJ| 26584| 20|20444|1102492295|\n",
      "|95f8944b-48a5-406...|      Evelyn Hampton|        IT|   RJ| 27185| 40|14750|1507670466|\n",
      "|a60e913d-f217-4e2...|      Joseph Griffin|        IT|   CA| 63632| 54|33758|1026610775|\n",
      "|d929b74e-325b-40f...|          Eric Jones|        IT|   NY|133685| 59|44636|1625113254|\n",
      "|4618ed6c-5c18-4f8...|      Connie Carlson|        HR|   FL| 80175| 54|59267|1252034639|\n",
      "|7b16c797-d007-474...|   Matthew Mccormick|     Sales|   RJ| 87586| 20|65558|1438642154|\n",
      "|22e61ad1-9b30-4c4...|     Jennifer Walker| Marketing|   CA| 56077| 25|21944|1586737722|\n",
      "|b8e9e7ff-1034-4dd...|      Rebecca Harvey|        HR|   IL| 10484| 57|72672|1079490827|\n",
      "+--------------------+--------------------+----------+-----+------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "(\n",
    "    kafka_json_df\n",
    "    .select(\n",
    "        from_json(col(\"value\"), schema)\n",
    "        .alias(\"data\")\n",
    "    )\n",
    "    .select(\"data.*\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stream Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f'{kafka_host}:9092')\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "parsed_df = (\n",
    "    kafka_df\n",
    "    .withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    .select(\n",
    "        from_json(col(\"value\"), schema)\n",
    "        .alias(\"data\")\n",
    "    )\n",
    "    .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    parsed_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    # .trigger(processingTime='5 seconds')\n",
    "    # .trigger(continuous='1 second')\n",
    "    # .trigger(once=true)\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
